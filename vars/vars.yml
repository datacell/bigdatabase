---
#Enable proxy
proxy_optional: False
proxy_server: <your company proxy server>
proxy_port: <your company proxy port>
proxy_user: <your user name>
proxy_passwd: <your password with scape charaters if any>
proxy_env:
  none: false
get_url_proxy_env:
  none: false


#Enable custom IP check
custom_ip_check: True
custom_ip_map_start: 205.28.128

build_name_: barty
build_version_: 1.0.0

package_download_dir: /vagrant/resources
#Status file dir
stat_dir: /etc/ipcomp
# log dir name complete path will be /var/log/components/*/*.log
logDir: /var/log/components

#Webserver
webserver_hdfs_path: /vagrant

#Python
setup_anaconda: True                             #Options True|False
anaconda_python_version: 2                       #Options 2|3
anaconda_version: 5.0.1
anaconda_parent_dir: /opt
anaconda_make_sys_default: True                  #Options True|False


#monit
setup_monit: False                               #Options True|False


#NodeJs
node_version: 8.9.0

#postgresql
postgresql_port: 5432
psqlUser: root
psqlPassword: root
setup_psql: True                                 #Options True


#redis
redis_port: 6379
setup_redis: True                                #Options True|False


#Hadoop Cluster Deployment
# Hadoop Deployment user
hadoop_user: hadoop
hadoop_dir: /opt/hadoop

#Hadoop
hadoop_version: 2.7.2

#Hive
hive_version: 1.2.1
hive_database_name: hive
hive_user: root
hive_password: root
hive_schema: "hive-schema-1.2.0.postgres.sql"
#Currently supported only with Postgresql. Setting False will result in hive failure.
hive_with_postgresql: True                       #Options: True


#Scala
scala_version: 2.11.6


#Spark
spark_version: 1.6.0
spark_hadoop_verson: 2.7
#Verify the avalable version file at
#http://archive.apache.org/dist/spark/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-hadoop{{ spark_hadoop_verson }}.tgz
#Below ports are mapped from CDH defaults
spark_master_web_ui_port: 18080
spark_history_server_port: 18088
spark_worker_web_ui_port: 18081
